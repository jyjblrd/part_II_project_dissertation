% This chapter should describe what was actually produced: the programs which were written, the hardware which was built or the theory which was developed. Any design strategies that looked ahead to the testing stage should be described in order to demonstrate a professional approach was taken.

% The repository overview should be around one page in length and should describe the high-level structure of the source code found in your source code repository. It should describe whether the code was written from scratch or if it built on an existing project or tutorial.

% Contribution to the field, with genuine potential for impact outside the tripos.

% Challenging goals and substantial deliverables, all methods and tools deployed expertly.

% Original techniques or methodologies going beyond what was previously known.

% Presentation is clear and concise throughout, with creative use of figures or diagrams.

% Excellent repository overview, giving clear insight into project structure.

\label{sec:3}

\section{Agent Architectural Overview}
\label{sec:architectural-overview}

\begin{figure}[h]
    \centering
    \includegraphics[trim=5cm 5cm 5cm 5cm, scale=0.2]{figures/agent_diagram.pdf}
    \caption{Agent diagram. Green arrows represent messages sent over ROS topics, while black arrows represent internal communications within a node.}
    \label{fig:agent-diagram}
\end{figure}

\autoref{fig:agent-diagram} gives an architectural overview of an agent, showing how the \texttt{Abstract Robot}, \texttt{SLAM System} and \texttt{Motion Controller} ROS nodes interconnect.

From a high level, we have an \texttt{Abstract Robot} node that provides an interface to the robot's hardware, as explained in \autoref{sec:ros-2}. This sends camera images to the \texttt{SLAM System} node, which builds a map of the world in collaboration with its peers. The \texttt{Motion Controller} node receives agent pose information from both the local \texttt{SLAM System} and the external peers to perform tasks such as collision avoidance by sending velocity commands back to the \texttt{Abstract Robot} node, closing the control loop.

In the following sections, we will explore these nodes in detail.

\section{SLAM System}
\label{sec:slam-system}
The SLAM System node is the majority of this project's implementation. It processes monocular images from the camera to localize the agent while also collaboratively building up a map of the world with its peers. As discussed in \autoref{sec:starting-point}, my system is based on an existing single-agent SLAM system that performs the \texttt{Tracking} and \texttt{Mapping} tasks. While substantial modifications were made to the base single-agent system, I will generally focus on the decentralized layer I have built on top in the interest of space.

\subsection{Datastructures}
\label{sec:datastructures}
A brief discussion of my SLAM system's internal data structures is required to understand the following sections. \autoref{fig:datastructure-diagram} shows the two key data structures: the map database and the visual word set. The map database contains multiple maps, which primarily consist of keyframes and map points. Keyframes are "snapshots" of the world, containing the predicted pose of the agent at that time along with the observed world features which we call "map points". These map points can be observed by multiple keyframes, as shown in \autoref{fig:datastructure-viz}, and when this happens we connect the keyframes with a "co-visible" link.

Keyframes also have multiple "visual words", which are vectors describing the visual contents of the keyframe. These are essential for detecting potential map merges, so we also build a "visual word set" that allows us to find all keyframes that contain a particular visual word.


\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[trim=5cm 4cm 5cm 5cm, width=\textwidth]{figures/datastructure_diagram.pdf}
        \caption{Data structure diagram}
        \label{fig:datastructure-diagram}
    \end{subfigure}\hfill%
    ~
    \begin{subfigure}[b]{0.425\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/datastructure_viz.pdf}
        \caption{Visual representation of data structure}
        \label{fig:datastructure-viz}
    \end{subfigure}%
\end{figure}

\subsection{Decentralized System Manager}
\label{sec:decentralized-system-manager}
Decentralized collaborative SLAM systems have significantly more complexity than a single-agent or even centralized collaborative system, due to the complex interactions between agents as they merge maps, lose localization, or lose connection with the network. Therefore, a robust framework must be put in place to ensure the robustness and correctness of the system, which I have implemented in the \texttt{Decentralized System Manager} component.

For the sake of simplicity, the following explanations will explore the interactions between just two agents: a \textit{local} and \textit{external} agent. In the \textit{\nameref{sec:generalizing-to-n-geq-3-agent-systems}} Section, we will show how this easily generalizes to a system with an arbitrary number of agents when augmented by some simple rules.

\subsubsection{State Manager}
\label{sec:state-manager}

\begin{figure}[h]
    \centering
    \includegraphics[trim=5cm 5cm 5cm 5cm, scale=0.2]{figures/slam_system_state_machine.pdf}
    \caption{SLAM system state machine for a single peer.}
    \label{fig:state-machine}
\end{figure}

Each agent's \texttt{Decentralized System Manager} maintains a state machine for every peer in the system, shown in \autoref{fig:state-machine}. All peers are initially in the \texttt{unmerged} state, meaning that they are in different coordinate frames and can not collaboratively build a map together. As the local agent starts to explore the same places as its peers, we are able to recognize the visual overlaps and merge their maps, bringing us to the \texttt{merged} state where the agents share the same coordinate frame and map, enabling relative positioning and collaborative map building.

\subsubsection{External Map Merge Finder}
\label{sec:external-map-merge-finder}
A naive approach to merging maps with an external agent is to constantly exchange our full maps, each time trying to identify if a map merge is possible. While simple, this approach clearly does not scale well from both a networking and computational perspective, as maps are often \>1MB in size and computing a full map merge is extremely computationally expensive.

Instead, we first identify if a map merge is even feasible by using visual words before attempting a full map merge. This eliminates superfluous map merge attempts that have no chance of succeeding because the agents' maps have no visual overlap.

As the agents generate new keyframes, we use DBoW2 \autocite{GalvezTRO12} to calculate the \hyperref[sec:visual-bag-of-words]{visual bag of words} seen by that keyframe and send them to our peers over the \texttt{/new\_key\_frame\_bows} ROS topic. These visual words are significantly smaller than sending over the complete map (as shown in \autoref{fig:kf-vs-bow-msg-size}) and enable agents to detect if there is a significant amount of visual overlap between its local map and the external agent's map. This is computed by \autoref{alg:map-merge-finder}.

This method gives a recall of almost 100\%, at the cost of potentially lower precision. This is a worthwhile tradeoff however, as it is essential to have very few false negatives so we can merge maps as soon as possible so the agents can begin collaborating and determine relative positioning.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/apr20_mh_trajectory_g_kf_vs_bow_msg_size.pdf}
    \caption{Comparison of the message size difference between a raw keyframe and a bag of words representation of a keyframe.}
    \label{fig:kf-vs-bow-msg-size}
\end{figure}

\begin{algorithm}
    \caption{Map merge finder using visual words. TODO: improve}
    \label{alg:map-merge-finder}
    \begin{algorithmic}[1]
        \Require{$E$: Set containing external keyframe's visual words}
        \Ensure{$success$: Boolean value signaling if a merge is possible based on visual words}
        \State $(externalMergeScore,\ bestMatchKeyFrame) \gets CalculateMergeScore(E)$
        \State $I \gets ComputeVisualWords(bestMatchKeyFrame)$
        \State $(internalMergeScore,\ _) \gets CalculateMergeScore(I)$
        \State $success \gets externalMergeScore \geq internalMergeScore$
    \end{algorithmic}
\end{algorithm}

\subsubsection{External Map Merger}
\label{sec:external-map-merger}
Performing full map merges is perhaps the most important component of the decentralized SLAM system, as a bad map merge will make it impossible for agents to collaborate. Additionally, it is one of the most computationally intensive parts of this system, therefore, it has gone through numerous iterations to optimize performance and reduce map merge errors.

The final version of the external map merger works as follows:

\begin{enumerate}
    \item \textbf{Request external agent's full map.} \\
          Once a potential map merge with the external agent is identified by the \hyperref[sec:external-map-merge-finder]{external map merge finder}, we can begin a full merge attempt. We first request the full map from the external agent through the \texttt{/get\_current\_map} ROS topic. Once the map is received, we deserialize it and place it into our \textit{map database} data structure as a seperate map.
    \item \textbf{Confirm that a map merge is possible using the full map data.} \\
          The visual word map merge finder works well considering its very low communication overhead, however we need to confirm that a map merge is actually possible using the full map data from both agents.

          TODO

          If we determine the merge to not be possible, we delete the external map from our map database and exit this process.
    \item \textbf{Find the translation $T_{loc \rightarrow ext}$ that aligns the local map to the external map.} \\
          TODO
    \item \textbf{Apply translation $T_{loc \rightarrow ext}$ to our local map.} \\
    \item \textbf{Move a subset of the external map's keyframes and map points into our local map} \\
          Given the external keyframe $k_0$ whose visual words triggered this map merge attempt, we extract a \textit{local window} $K$ of keyframes connected to $k_0$ in the co-visibility graph.
    \item \textbf{Merge local and external map points, connecting the maps} \\
    \item \textbf{Run bundle adjustment to optimize map point and keyframe positions.} \\
    \item \textbf{Repeat steps 5-7 with the entirety of the external map.} \\
    \item \textbf{Broadcast successful merge message.} \\
          If the full map merge is ultimately successful, we broadcast a \texttt{/successfully\_merged} message to tell the external agent that we have successfully merged their map. The external agent will then move to the \texttt{merged} state and both agents will begin sharing keyframes with each other.

\end{enumerate}

By the end of the merge process, the local agent will change its coordinate frame to that of the external agent.

It is important to note that this system requires only one agent to identify and calculate the map merge, significantly reducing the computational overhead of map merging, especially in systems with many agents (further explained in \autoref{sec:generalizing-to-n-geq-3-agent-systems}).

\subsubsection{External KeyFrame Inserter}
\label{sec:external-key-frame-inserter}
Once the local and external agents have merged their maps and share the same coordinate frame, they can begin sharing keyframes with each other.

Each agent maintains a set of sent keyframes $K_{sent}$ and map points $M_{sent}$. The set of unsent keyframes and map points are therefore represented as $K_{unsent} = K / K_{sent}$ and $M_{unsent} = M / M_{sent}$ respectively. Once $\#K_{unsent}$ exceeds a certain threshold, we serialize $K_{unsent}$ and $M_{unsent}$ and send them to the external agent. Finally, we add $K_{unsent}$ to $K_{sent}$ and $M_{unsent}$ to $M_{sent}$

Upon receiving the serialized keyframes and map points, the external agent deserializes them and adds them to a queue to await insertion into their local copy of the shared map by the \textit{external keyframe inserter}.

The external keyframe inserter is run whenever we have spare cycles on the CPU, to prevent impacting the local tracking and mapping performance. The insertion process involves the following operations:

\begin{figure}[h]
    \centering

    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[height=1.9in]{figures/external_key_frame_insertion_1.pdf}
        \caption{External keyframe $k_{ext}$ with $M_{ext}=\{m_3, m_4, m_5\}$. Note the references to existing keyframes and map points.}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[height=1.9in]{figures/external_key_frame_insertion_2.pdf}
        \caption{Existing local map.}
    \end{subfigure}%
    \par\bigskip
    \begin{subfigure}[t]{0.333\textwidth}
        \centering
        \includegraphics[height=1.9in]{figures/external_key_frame_insertion_3.pdf}
        \caption{\textbf{Step 2, 3\&4}: Move $k_{ext}$ and $M_{ext}$ to the local map and relink references. Relinked connections are drawn in purple.}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.333\textwidth}
        \centering
        \includegraphics[height=1.9in]{figures/external_key_frame_insertion_4.pdf}
        \caption{\textbf{Step 5}: Merge duplicate map points. $m_5$ and $m_0$ have a similar feature descriptor and location, therefore they are merged.}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.333\textwidth}
        \centering
        \includegraphics[height=1.9in]{figures/external_key_frame_insertion_5.pdf}
        \caption{\textbf{Step 6}: Local bundle adjustment around $k_{ext}$ to refine the map using the new information. Original keyframe and map point locations shown in purple.}
    \end{subfigure}%

    \caption{Visual overview of inserting external keyframes and map points into the local map. External keyframe (a) and initial local map (b) are combined to create our final local map (e).}

\end{figure}

\begin{enumerate}
    \item \textbf{Pop external keyframe $k_{ext}$ from front of queue}.
    \item \textbf{Move $k_{ext}$ and its new external observed map points $M_{ext}$ to the local map}.
          Since the local and external agents are merged and in the same coordinate frame, we can simply move $k_{ext}$ and $M_{ext}$ to our local map without any transformations.\footnote[1]{I had previously used a \textit{keyframe anchor} method, where instead of $k_{ext}$ having an absolute pose we would send its pose relative to the previous $k_{ext}$. The thought process behind this was to prevent minor misalignments between the local and external maps from preventing external keyframes from properly integrating with the local map. However, experimental testing showed that this method instead caused the local and external maps to frequently diverge.}
    \item \textbf{Relink $k_{ext}$ with co-visible keyframes and observed map points in the local map}. \\
          $k_{ext}$ contains references to its covisible keyframes and child map points that have already been sent or were generated by another agent. We search our local map for objects that match these references, reconnecting them. More details of the deserialization process are given in \autoref{sec:map-serialization-and-deserialization}.
    \item \textbf{Relink $M_{ext}$ with keyframes in the local map which observe it}. \\
          $M_{ext}$ contains references to keyframes which observe it which have already been sent or were generated by another agent. We search our local map for keyframes that match these references, reconnecting them. More details of the deserialization process are given in \autoref{sec:map-serialization-and-deserialization}.
    \item \textbf{Merge $M_{ext}$ with map points in the local map}. \\
          $M_{ext}$ will already be correctly linked to observing keyframes in the local map, however, due to communication latency some map points in $M_{ext}$ may be duplicates of existing map points in the local map. Therefore, we exploit spatial locality to combine duplicate map points that describe the same physical feature. A key observation was that this step is essential to ensuring the local and external keyframes stay well connected, ensuring the local and external maps do not diverge.

          TODO: add more details? idk
    \item \textbf{Perform a local bundle adjustment around $k_{ext}$}. \\
          Since we have merged map points in the previous step, we need to perform bundle adjustment to tweak the keyframe and map point locations to minimize reprojection error. This helps create a more accurate map, minimizing tracking error. We also only need to perform this bundle adjustment locally, greatly reducing the computational cost.
\end{enumerate}



\subsubsection{Local KeyFrame Inserter}
\label{sec:local-key-frame-inserter}
As mentioned in the \nameref{sec:external-key-frame-inserter} section, it is essential that the local and external maps stay well connected, sharing the majority of their map points. In other words, we must use the external map points when running the 7 point algorithm (TODO: ref section?) to generate new local keyframes.

The \textit{Tracking} and \textit{Mapping} modules, which are responsible for localizing the robot and generating new keyframes, only interact with the \textit{Map Database}. Our external keyframe inserter does all the work of properly reconecting external map points and merging duplicate map points, leaving the \textit{Map Database} datastructure looking as if all the map points and keyframes were generated locally. This essentially abstracts the \textit{Tracking} and \textit{Mapping} modules away from the distributed aspects of the system, meaning they fully use the external map points when localizing the agent and generating new keyframes.

\subsection{Generalizing to $N \geq 3$ Agent Systems}
\label{sec:generalizing-to-n-geq-3-agent-systems}
Now that we have explained how a pair of agents interact, we can explore how this can be generalized to a system with an arbitrary number of agents.

We assume a system with $N$ agents $A=\{agent_1, agent_2, ..., agent_N\}$, where $agent_i$ is the agent with ID = i. Within this system we maintain a state $S_i=state_{n-m}$ for every agent pair $(agent_n, agent_m)$, giving us a total of $N(N-1)/2$ states. Conceptually, this is a fully connected graph with agents as the nodes and states between agents as the edges. We also have a set $G$ which contains all the groups of merged agents. The \textit{group leader} is defined as the agent in a group with the lowest agentId.

In the case where agents can lose communication with one another, we also assume that if any given $agent_n$ is able to communicate with an $agent_m$, $agent_n$ can also communicate with all of $agent_m$'s connected peers. This is held if the agents are using a mesh network to communicate, for example.

Initially, every agent pair is unmerged so every state in $S$ is set as \textit{unmerged} and $G=\{\{agent_1\}, \{agent_2\}, ..., \{agent_N\}\}$

A key insight of my distributed SLAM system is to delegate all merge operations to group leaders. This means that instead of all agents attempting merges with every other agent, only group agents have to attempt merges amongst themselves. This is significant, as the computational load of these merge operations scale proportional to the square of the number of agents involved, and in most use cases the number of group leaders quickly drops to be much lower than the total number of agents.

Additionally, having all merge operations performed by the group leader prevents potential race conditions introduced by communication latency within a group, for example two agents within a group both merging with different agents at the same time.

As discussed in the \hyperref[sec:external-map-merge-finder]{External Map Merge Finder} and \hyperref[sec:external-map-merger]{External Map Merger} Sections, the two major operations needed to merge are (1) exchanging visual words to identify visual overlap and therefore merge opportunities and (2) sending over the map and attempting a full map merge.

Tackling (1) first, instead of the group leader only sending the visual words of its own keyframes, it will send the visual words of all keyframes generated by agents within its group. Keep in mind that it will only be sending these visual words to other group leaders. This introduces no additional intra-group communication, as all agents within a group already send each other all new keyframes.

Moving on to (2), once a pair of group leaders $(agent_n, agent_m)$ (with $n<m$) have successfully merged their maps, the agent with the larger ID ($agent_m$) will change its coordinate frame to the agent with the smaller ID ($agent_n$). $agent_m$ will then send the transform from $agent_m$ to $agent_n$'s coordinate frame to the other agents in its group, allowing them to also change to $agent_n$'s coordinate frame. After this has been completed, $agent_m$'s group merges into $agent_n$'s group using \autoref{eq:1}, and agents begin exchanging keyframes to update each other's maps. Eventually, once all agents are merged together, they will all be in $agent_0$'s coordinate frame.

Agents are able to keep track of the current groups and group leaders within the decentralized system since all successful merge messages are broadcast on the shared \texttt{/successfully\_merged} ROS topic.

TODO: fix obviously
\begin{gather} \label{eq:1}
    i \in G_n, j \in G_m, state_{i-j} \in S.\ state_{i-j} = merged\\ \text{where $G_n \in G$ and $G_m \in G$ are $agent_n$ and $agent_m$'s groups respectively.}
\end{gather}

This is perhaps best represented in visual form by the simple 3 agent merge example given in \autoref{fig:3-agent-merge} which shows the messages sent between the agents. This example is slightly simplified as all potential merges result in successful full merges. In reality, multiple potential merges may be identified before two agents successfully merge.

\begin{figure}[h]
    \centering
    \includegraphics[]{figures/3_agent_merge.pdf}

    \caption{This is a simple 3 agent merge example where $agent_1$ and $agent_2$ merge first, and then $agent_0$ and $agent_1$. Agents in the same group are shown in the same rectabgle, and the group leader is bolded. \captionbreak Initially at $t_0$, all agents are unmerged and therefore share the bag of words representations of new keyframes with the other agents. A potential merge with $agent_1$ is then detected by $agent_2$. Since $2>1$, $agent_2$ needs to perform the merge so it request $agent_1$'s full map. It successfully merges with $agent_1$'s full map and broadcasts this information to all other agents. \captionbreak At $t_1$, $agent_1$ and $agent_2$ are merged therefore they begin exchanging keyframes. $agent_0$ and $agent_1$ are the two remaining group leaders, so they continue to exchange they new keyframe's bag of words. $agent_0$ detects a potential merge, and since $0<1$ it sends its full map to $agent_1$. $agent_1$ is able to successfully merge with $agent_0$'s full map, so it broadcasts that $agent_0$ and $agent_1$. Additionally, it sends the transform $T_{1 \to 0}$ ($agent_1$ to $agent_0$'s coordinate frame) to $agent_2$ allowing it to change to $agent_0$'s coordinate frame. $agent_1$ then broadcasts that $agent_0$ and $agent_2$ have implicitly merged. \captionbreak At $t_2$, all three agents are merged and in $agent_0$'s coordinate frame, and therefore they all exchange keyframes with one another.}
    \label{fig:3-agent-merge}
\end{figure}

\subsubsection{Map Alignment Refiner}
\label{sec:map-alignment-refiner}
As our shared map grows, the maps stored locally by the agents may "fall out of alignment". By this, we mean that the maps are slightly translated, rotated, or scaled with respect to the lead agent's map. This is largely a side effect of our aggressive early merge strategy which may merge two agents' maps before there is significant overlap, causing the estimated map alignment (ie. transformation between the two agents' origins) to have some error.

These small alignment errors are completely acceptable when maps are small, but may cause the maps to diverge as they grow.

To remedy this problem, we continuously refine our map alignment using \nameref{sec:ransac} and the Kabsch-Umeyama point alignment algorithm. Map alignment is performed as follows:

\begin{enumerate}
    \item \textbf{Request map point locations from the lead agent}. \\
          This is defined as the set $TaggedMP_{ext}$ where ${TaggedMP_{ext}}_i = (uuid, (x, y, z))$
    \item \textbf{Extract local map point locations}. \\
          This is defined as the set $TaggedMP_{local}$ where ${TaggedMP_{local}}_i = (uuid, (x, y, z))$
    \item \textbf{Use the Kabsch-Umeyama and RANSAC algorithms to find transform $T$ which best aligns $TaggedMP_{ext}$ to $TaggedMP_{local}$.}
          The Kabsch-Umeyama algorithm finds the SIM(3) transformation $T$ from $TaggedMP_{ext}$ to $TaggedMP_{local}$, minimizing the RMSE. However, our input data has a large number of outliers so we use RANSAC on top to find a good fit while ignoring outliers.

          The RANSAC and Kabsch-Umeyama algorithms are described in detail in \autoref{sec:ransac} and \autoref{sec:kabsch-umeyama-algorithm} respectively.
    \item \textbf{Apply transformation $T$ to our local map.}
\end{enumerate}

We use an \textit{additive increase multiplicative decrease} methodology to control how often map alignment is performed. Given $t_i$ is the time between the $i$-th and $(i+1)$-th map alignments, we set $t_{i+1} = t_i + 1$ if the maps were well aligned, and $t_{i+1} = t_i / 2$ if the maps were not well aligned. This prevents agents from continuously performing map alignments when their maps are already well aligned.

\subsubsection{Losing Localization / Connection}
\label{sec:losing-localization-or-connection}
An agent can lose localization within the map, for example, if the camera is blocked for a short period of time. When this happens, the agent signals to the other agents that it has lost localization and they temporarily stop sharing new keyframes. When the agent relocalizes itself, keyframe sharing resumes and the agents send all the stored-up unsent keyframes.

Losing connection is very similar, where keyframe exchanging is stopped until the connection is regained, at which point the agents update each other with the stored-up unsent keyframes.

\subsubsection{Visualization Publisher}
\label{sec:visualization-publisher}

\subsection{Map Serialization and Deserialization}
\label{sec:map-serialization-and-deserialization}
Map serialization and deserialization are essential and non-trivial components of this SLAM system, allowing agents to share their maps across the network. For this task, I used the Boost \autocite{boostLibrary} Serialization C++ library since it supports standard library collections and other common classes.

As discussed in \autoref{sec:slam-system}, maps contain keyframes and map points. Individually, these objects are relatively easy to serialize with boost – in fact, the single agent SLAM program my system is based on already supported saving and loading maps allowing me to leverage some of their serialization helper functions. To serialize these objects we simply need to define a serialization scheme for each custom class, describing which class variables should be serialized and which shouldn't. Strategically selecting only the parameters that need to be serialized allows us to cut back on communication overhead. For example, there is no need for us to serialize a keyframe's raw image, as it is not used in the rest of our SLAM pipeline.

Complexities arise when we try to serialize/deserialize the connections between these objects, especially when we are only sending map fragments. For example, if we send a new keyframe $k$ and its map points $M_k$ to an external agent, due to the many-to-many relationship between keyframes and map points, some of the map points in $M_k$ may have already been sent before. We can define $M_k = M_{k\_sent} \cup M_{k\_unsent}$, where $M_{k\_sent}$ are the map points that have already been sent to the external agent and $M_{k\_unsent}$ are the ones that have not. Map points in $M_{k\_sent}$ will need to be connected to $k$ when it is deserialized by the external agent, and map points in $M_{k\_unsent}$ will need to be sent to the external agent and connected to any existing keyframes in the external map that observe the map points.

We manage these connections by giving every keyframe and map point a universal unique identifier (UUID), allowing us to use these UUIDs as references to a specific object in our multi-agent system. The beauty of UUIDs is that they do not require a centralized server to assign a unique ID to every object. Instead, we can generate the unique IDs without any communication with our peers\footnote[1]{While not \textit{verifiablly} unique, UUIDs are unique within practical limits. A commonly cited anecdote is that it is far more likely for a cosmic ray to cause a bug than a UUID collision.}.

This method of using UUIDs as references is used to rebuild all connections after deserialization, including the keyframe-to-keyframe connections that build the co-visibility graph and many others that I have not had the space to discuss in this dissertation.

\section{Motion Controller}
\label{sec:motion-controller}
While not a part of the core SLAM system, the motion controller node closes the control loop and demonstrates the real-world usability of my system. From a high level, the motion controller node consumes the local and external agents' poses and outputs a command velocity to the robot's movement system – all via ROS topics. For this project, I have implemented two different motion control systems which can be switched out seamlessly.

\subsection{Follow The Leader}
\label{sec:follow-the-leader}
The "follow the leader" motion controller consists of two agents: one leader and one follower. The follower is given a position and rotation offset to the leader which it attempts to maintain as the leader is moved around.

For example, you could set the follower to be 1 meter behind the leader and with a 180\textdegree{} rotational offset. This ensures that the leader and follower have no visual overlap, demonstrating that my SLAM system is truly building a shared map.

Spinning the agents around in a circle demonstrates that the two agents' maps are properly merged and that the agents are using map points generated by an external agent to localize themselves, giving accurate relative positioning even when there is no visual overlap at any given moment.

TODO: add figure of the map built

\subsection{Multi-Agent Collision Avoidance}
\label{sec:multi-agent-collision-avoidance}
The "multi-agent collision avoidance" example is more complex, employing a non-linear model predictive controller (NMPC) to avoid collisions with both static and dynamic obstacles. My NMPC system is derived from \autocite{DBLP:journals/corr/KamelASN17} and is defined as follows:

\subsubsection{Non-Linear Model Predictive Controller Formulation}
\label{sec:nmpc-implementation-details}
We assume an agent with current pose $\bm{p} \in \mathbb{R}^2$ and radius $r$. Firstly, we define our agent's control input $\bm{v_{cmd}}$ as a function describing its velocity over time. We can then define our agent's state $\bm{x}$ as the control input $\bm{v_{cmd}}$ applied to its current pose $\bm{p}$:

\begin{equation}
    \bm{x}(t) = \bm{p} + \int_{0}^{t} \bm{v_{cmd}}\ dt
\end{equation}

We can then solve the following minimization problem to find an optimal $\bm{v_{cmd}}$:
\begin{equation}
    \begin{aligned} \label{eq:nmpc-minimization-problem}
         & \min_{\bm{v_{cmd}}} \int_{t=0}^{T} J_x(\bm{x}(t), \bm{x_{ref}}(t)) + J_s(\bm{x}(t)) + J_d(\bm{x}(t))\ dt \\
         & \text{subject to } \bm{v_{min}} \leq \bm{v_{cmd}} \leq \bm{v_{max}}
    \end{aligned}
\end{equation}
where $T$ is our horizon length, $\bm{x_{ref}}$ is our target trajectory, and $J_x$, $J_s$, $J_d$ are the cost functions for this system.

Cost function $J_x$ rewards following the target trajectory $\bm{x_{ref}}$ and is shown below:
\begin{equation}
    J_x(\bm{x}, \bm{x_{ref}}) = \|\bm{x} - \bm{x_{ref}}\|^2
\end{equation}

$J_s$ and $J_d$ penalize collisions with static objects and dynamic objects respectively. They are defined as:
\begin{flalign}
    J_s(\bm{x}) & = \sum_{i=1}^{N_{static}} \frac{s_s*Q_s}{1 + \exp(d_i^{static} / s_s)}   \\
    J_d(\bm{x}) & = \sum_{i=1}^{N_{dynamic}} \frac{s_d*Q_d}{1 + \exp(d_i^{dynamic} / s_d)}
\end{flalign}
where $d_i^{static}$ is the distance between the agent and the $i-th$ static obstacle, and $d_i^{dynamic}$ is the distance between the agent and the $i-th$ dynamic obstacle. For example, if the $i-th$ dynamic obstacle is another agent with radius $r$ then $d_i^{dynamic} = \|\bm{x}(t)-\bm{x_i}(t)\|^2 - r$. $Q_s>0$ and $Q_d>0$ are weights that define how far the agent stays away from static obstacles compared to dynamic ones.

Additionally, we define $s_s$ and $s_d$ as a scale normalizing parameter that ensures the minima of $J_x + J_s$ and $J_x + J_d$ are greater than agent radius $r$ in the single obstacle case. Essentially, this ensures that the optimized trajectory of the agent never comes within radius $r$ of an obstacle unless it is being "squished" by two obstacles.

To calculate $s_s$, we first find the positive minima $x_{min}$ of $J_x + J_s$ in the worst case where $d_i^{static} = J_x$ (ie. the static obstacle and $x_{ref}$ are in the same place), with $s=1$. This gives us $x_{min}$ as defined in \autoref{eq:x-min}. Our scaling factor can then be defined as $s_s = \frac{r}{x_{min}}$, which sets the positive minima of $J_x + J_s$ to be $r$ in the above situation. The calculation of $s_d$ is symmetric, simply using $Q_d$ instead of $Q_s$.

\begin{equation} \label{eq:x-min}
    x_{min} = \ln \left( \frac{\sqrt{Q_s^2-4Q_s}+Q_s}{2}-1 \right)
\end{equation}

The key benefit of these cost functions is that the optimal distance to obstacles of $r$ is invariant to parameters $Q_s$ and $Q_d$. This is in contrast to \autocite{DBLP:journals/corr/KamelASN17} which requires you to retune the parameters $\kappa$ and $Q$ upon changing agent radius $r$. This is because their cost functions do not have their minima at the collision point, but instead at some point before the collision that varies with $\kappa$, $Q$, and $r$, making tuning very difficult. This is visualized in \autoref{fig:nmpc_cost_function}

\begin{figure}[h]
    \centering

    \begin{subfigure}[t]{0.33\textwidth}
        \centering
        \caption{My Cost Function}
        \includegraphics[height=1.7in]{figures/nmpc_cost_function_0.pdf}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.33\textwidth}
        \centering
        \caption{Kamel et al.'s Cost Function}
        \includegraphics[height=1.7in]{figures/nmpc_cost_function_1.pdf}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.33\textwidth}
        \centering
        \caption{Kamel et al.'s Cost Function}
        \includegraphics[height=1.7in]{figures/nmpc_cost_function_2.pdf}
    \end{subfigure}%

    \caption{Comparison of my cost function (a) and Kamel et al.'s cost function (b), (c) when the obstacle and goal pose are at the same location. This demonstrates that my cost function's minima is always at the collision point and does not vary as the parameters are changed, as opposed to Kamel et al.'s function.}
    \label{fig:nmpc_cost_function}
\end{figure}

\subsubsection{Implementation Details}
\label{sec:nmpc-implementation-details}
Solving the integral in \autoref{eq:nmpc-minimization-problem} is computationally inefficient, therefore we split our calculations into discrete time steps. Specifically, the time horizon is set as $T=TODO$ with $TODOms$ timesteps.

The Sequential Least Squares Programming minimization method is used due to its robustness and ability to constrain the optimization space. We set parameters $Q_s = TODO$ and $Q_d = TODO$ to make the agent more adverse to approaching dynamic obstacles compared to static ones, as there is more uncertainty in the dynamic obstacle's location. To calculate the future locations of dynamic obstacles $x_i^{dynamic}$ we employ a simple constant velocity model.

Our SLAM system does not use a depth camera or LiDAR, and therefore only produces a sparse map. Therefore, we need to define the location of static obstacles manually. Both static obstacles and the goal pose are set using interactive ROS markers, allowing them to be changed on the fly using software such as RViz.

\section{Central Management Interface}
\label{sec:central-management-interface}
While my multi-agent system is fully decentralized, a significant amount of work was put into developing the supporting infrastructure needed to control, test, and evaluate this system.

This was part of a broader aim of this project to create more than just a research project that only runs on my machine. I have strived to develop a system that can be deployed in real-world use cases, and the significant infrastructure released alongside my SLAM system hopes to help achieve that goal.

The primary method of managing the distributed system is through the \textit{Central Management Interface}, which can be used to: \noparskip
\smallbreak
{
    \begin{enumerate}
        \item \textbf{Manually control the agent's poses.} \\
              Users can click and drag on an image in area 1 to rotate the agent's camera and use the keyboard to move the agent around.
        \item \textbf{Record camera data from the simulation software.}
        \item \textbf{Play datasets back for testing and benchmarking purposes.} \\
              Additionally, there are settings to remap topics and control the speed of playback.
        \item \textbf{Record trajectories generated by the SLAM system as well as ground truth data for later evaluation.} \\
              The recorded trajectory data can later be ingested by my custom evaluation library Multi-Agent Evo for analysis.
    \end{enumerate}
}

\begin{figure}[h]
    \centering
    \begin{subfigure}[c]{0.425\textwidth}
        \centering
        \includegraphics[trim=5cm 5cm 5cm 5cm, width=\linewidth]{figures/central_management_interface_diagram.pdf}
        \caption{Central Management Interface architectural diagram}
    \end{subfigure}\hfill%
    ~
    \begin{subfigure}[c]{0.525\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/central_management_interface.pdf}
        \caption{Central Management Interface}
    \end{subfigure}%

    \caption{Central management interface. Used for controlling agents in a simulation, recording datasets, playing back datasets, and recording trajectories for later analysis.}
    \label{fig:central-management-interface}
\end{figure}

The central management interface was built using the PyQt cross-platform user interface framework.

As a result of abstracting implementation details behind ROS topics, this central management interface is able to work seamlessly with both agents running in a simulator and agents running on real-world robots. This is true of every component of this project.

\section{Custom Evaluation Suite – Multi-Agent EVO}
\label{sec:multi-agent-evo}
While there are several mature single-agent SLAM evaluation tools, I found there to be a complete lack of evaluation tools for multi-agent SLAM systems – to the best of my knowledge no papers have published their evaluation code. Continuing with my goal to develop and publish transferrable infrastructure alongside this project, I have created an open-source multi-agent SLAM evaluation tool: \textit{Multi Agent EVO}, based on the popular single agent SLAM evaluation tool \textit{EVO} \autocite{grupp2017evo}.

Besides the simple data structure and data ingestion modifications needed to allow EVO to process multi-agent SLAM data, there is some additional nuance to evaluating data from multiple agents.

Initially, all agents will be in separate reference frames until they explore an area previously seen by another agent, allowing them to merge their maps and share the same coordinate frame. We may also have cases where two independent groups of agents meet and merge maps, which requires multiple agents to simultaneously change coordinate frames. We also must note that these coordinate frames are part of the SIM(3) transformation group, which is composed of rotation, translation, and uniform scale in 3-dimensional space (scale being necessitated by the scale ambiguity of monocular visual SLAM).

Therefore, I have created a new data format to capture these changes in coordinate frames over time within our trajectory data, which Multi-Agent EVO is able to ingest. This allows us to properly compare the multi-agent SLAM trajectories to the ground truth data, giving us insights into how long it takes for agents to successfully merge maps, the accuracy of relative pose estimation, and much more.

Once agents $m$ and $n$ merge, they publish the translation $T_{m \to n}$ to the \texttt{sim\_transform} ROS topic. When we go to evaluate the trajectory error, Multi-Agent EVO ingests the \texttt{sim\_transform} data to build a directed acyclic graph (DAG) of all merged agents, as shown in \autoref{fig:evo-coord-frames}. This DAG can then be used to find if agents are merged with each other, and the transform between their coordinate frames. We then transform all the trajectories to be in $agent_0$'s coordinate frame for us to perform error analysis on the joint trajectory.

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=0.5\linewidth]{figures/evo_1.pdf}
        \caption{All agents are initially unmerged.}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=0.5\linewidth]{figures/evo_2.pdf}
        \caption{$agent_0$ and $agent_1$ merge, $agent_2$ and $agent_3$ merge.}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=0.5\linewidth]{figures/evo_3.pdf}
        \caption{$agent_0$ and $agent_2$ merge.}
    \end{subfigure}%

    \caption{Example of how coordinate frames are managed in Multi-Agent EVO. A path from $agent_m$ to $agent_n$ signifies that the agents are merged and there is an available translation from $agent_m$'s coordinate frame to $agent_n$'s.}
    \label{fig:evo-coord-frames}
\end{figure}

This method allows us to retroactively transform trajectories into $agnet_0$'s coordinate from before the agents merged, allowing us to also analyze the quality of the pre-merge estimated relative trajectories.

\section{Simulation Environment}
\label{sec:simulation-environment}
The \hyperref[sec:webots-simulator]{Webots Simulation Software} Section in the Preparation Chapter explains the motivation behind using a simulator. Essentially, leveraging simulations enabled much faster iteration cycles and the ability to test in a variety of environments. This section will focus on the details of integrating the simulation software into my system.

Webots is an open-source 3D robotics simulator with realistic rendering, which is essential for testing a visual SLAM system. Webots can not be built for the ARM Ubuntu VM I used for development, therefore it had to be run on MacOS, and the data was streamed to the Ubuntu VM through a websocket bridge developed by Webots.

I then developed a ROS node that exposes the Webots cameras and agent controls as ROS topics, following the abstract robot interface specifications. This allowed my SLAM system to process the camera streams and the motion controller / central management interface to control the agent's movements.


\begin{figure}[h]
    \centering
    \includegraphics[trim=5cm 5cm 5cm 5cm, scale=0.2]{figures/simulation_environment.pdf}

    \caption{Simulation system architecture. The Webots application runs on MacOS and is exposed to the Ubuntu VM as ROS topics by the custom \texttt{Webots Client} ROS node.}
    \label{fig:simulation-environment}
\end{figure}

\section{Real World Implementation}
\label{sec:real-world-implementation}
After months of development within a simulator, my distributed SLAM system was able to seamlessly be deployed in a real-world multi-agent system. This was largely due to the decisions made during the preparation phase. For example, the choice to use ROS 2 as a communication middleware allows the agents to easily communicate even when deployed on different physical devices. Another key choice was to abstract the implementation of nodes away behind the communication layer, as it allowed my system to run on the physical robots without making any changes to the SLAM system or motion controller nodes.

\subsection{Cambridge RoboMaster Platform}
\label{sec:cambridge-robomaster-platform}
I chose to deploy my system on the Cambridge RoboMaster, which is an omnidirectional robot platform controlled by a NVidia Jetson Orin. The only sensor used on the RoboMaster was the integrated Raspberry Pi HQ Camera, which provides a 1920x1080 image at 15hz.

This was an obvious platform to use, as ROS drivers for the camera and wheels had already been developed and a ground-based platform allows for easier testing.

As an aside, \textbf{I was an author for the paper \textit{The Cambridge RoboMaster: An Agile Multi-Robot Research Platform}}, which has been submitted to the 17th International Symposium on Distributed Autonomous Robotic Systems. My distributed SLAM system running the multi-agent collision avoidance motion controller was included as one of the experiments analyzed in the paper.

TODO: add pic

\subsection{Deploying with Docker}
\label{sec:deploying-with-docker}
My SLAM system is deployed on the RoboMasters in Docker containers. This allows my code to be compartmentalized from other projects being developed on the RoboMasters, as they are actively used by numerous researchers in the Prorok Lab. Additionally, I have set up a continuous integration process on GitHub to build a Docker container for the RoboMasters on every code push, which can then be pulled to the RoboMasters. This is a very useful feature, as building my full codebase can take upwards of 20 minutes on the NVidia Jetson Orins.

\subsection{OptiTrack Motion Capture System}
\label{sec:optitrack-motion-capture-system}
The ground truth for my real-world experiments are provided by the Prorok Lab's OptiTrack Motion Capture System, which advertises accuracies of $\leq$0.3mm at rates of 180hz \autocite{OptiTrackForRobotics}. These motion capture system publishes the real-time poses of the tracked objects to a ROS topic which can be recorded for later analysis.

\subsection{Raspberry Pi Video Publisher}
\label{sec:raspberry-pi-video-publisher}
This dissertation also presents the Raspberry Pi Video Publisher, an independent piece of infrastructure which I have developed both the hardware and software for. This platform publishes real-time camera data to a ROS topic and can be tracked by the lab's motion capture system. The two primary use cases I have used this platform for are (1) dataset generation and (2) real-time AR visualization of 3D data (\autoref{sec:augmented-reality-visualization}).

As explained in the \nameref{sec:cicd} Section, the software for this platform is entirely dockerized and a CI/CD pipeline has been implemented to automatically build and deploy the software to all Raspberry Pi video publishers when new code is pushed to the GitHub repository. This is particularly useful, as it is designed as a plug-and-play system that starts streaming video data as soon as it is turned on.

The hardware for this platform consists of a Raspberry Pi 4b, Raspberry Pi Camera v2, 3D printed frame, and motion capture markers. The 3D-printed frame securely mounts the components together, allowing them to be carried around or mounted to a tripod.

TODO: add pic of rpi thing, 3d frame model

\subsection{Augmented Reality Visualization}
\label{sec:augmented-reality-visualization}
The Raspberry Pi Video Publisher platform is tracked by the motion capture system, allowing us to project 3D visualization data onto the captured video. This can be used to visualize the SLAM system's keyframe and map point locations in the real world and to view how the SLAM system's predicted trajectory aligns with reality.

To overlay our SLAM system's data on the video, we must first align the SLAM system and motion capture systems' coordinate frames. This needs to be done on every run, as monocular SLAM systems have an arbitrary scale. We therefore use the \hyperref[sec:kabsch-umeyama-algorithm]{Kabsch-Umeyama algorithm} to align the trajectories captured by the motion capture system and SLAM system after enough data has been collected to create a successful alignment. We can then use Foxglove Studio to draw our 3D markers and project them on top of the tracked camera's video stream.

To my knowledge, this system is the first of its kind to be used in such a project. Not only is it visually impressive, but it also can be used for further analysis and understanding. For example, the camera can be used to understand which features the SLAM system is tracking and how well their predicted location aligns with reality. Additionally, it can be used to analyze the SLAM system's predicted trajectory – this visualization helped me identify that my collision avoidance demo was being severely impacted by latency spikes which I proceeded to fix.

TODO: put visualization here

\section{Repository Overview}
\label{sec:repository-overview}
